{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df327e2-7503-484f-9e84-134f31f1a6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/938], Loss: 0.6323\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load FashionMNIST dataset\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data',\n",
    "                                                  train=True,\n",
    "                                                  transform=transform,\n",
    "                                                  download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data',\n",
    "                                                 train=False,\n",
    "                                                 transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "# Get a sample image and label\n",
    "image_tensor, label = train_dataset [0]  # image_tensor is normalized\n",
    "\n",
    "# Undo normalization for visualization\n",
    "# Normalization: x_norm = (x - mean) / std â‡’ x = x_norm * std + mean\n",
    "unnormalized = image_tensor * 0.5 + 0.5  # reverse normalization\n",
    "\n",
    "# Convert from tensor to numpy for imshow\n",
    "image_np = unnormalized.squeeze().numpy()  # shape: (28, 28)\n",
    "\n",
    "# Show image\n",
    "plt.imshow(image_np, cmap='gray')\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Class labels\n",
    "classes = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# CNN Model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, nesterov=True)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c2a10d-cf5b-476d-afc9-4c76f4d29e89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GAN)",
   "language": "python",
   "name": "gan-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
